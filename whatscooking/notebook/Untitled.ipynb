{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program configuration\n",
    "env = None\n",
    "sample_size = None\n",
    "# Set env, if env = test, will only be run locally and display the result\n",
    "env = \"prod\"\n",
    "env = \"test\"\n",
    "\n",
    "# Nb model to generate\n",
    "nb_models = 10\n",
    "# Nb value to filter, if less than, we remove those\n",
    "nb_min_value_before = 50\n",
    "\n",
    "normalize_training = False\n",
    "max_to_add = 1000\n",
    "\n",
    "# Number of value on which to train, if null, train on all value\n",
    "sample_size = None\n",
    "test_size = 1000\n",
    "nb_min_value_after = 200\n",
    "\n",
    "forbidden_regexp=\"[0-9]+\"\n",
    "\n",
    "# If false, value might persist, good to rerun, bad for memory\n",
    "garbage_collector = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On production environment, just use garbage collection\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "if garbage_collector or env == \"prod\":\n",
    "    os.environ['GC'] = 'true'\n",
    "else:\n",
    "    os.environ['GC'] = 'false'\n",
    "    \n",
    "def del_object(object):\n",
    "    if os.environ['GC'] == 'true':\n",
    "        del(object)\n",
    "        gc.collect()\n",
    "\n",
    "def pt(toPrint):\n",
    "    print('{0} - {1}'.format(datetime.now(), toPrint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data + test data\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(env, sample_size):\n",
    "    pt(\"start - read_data\")\n",
    "    df_data = pd.read_json(\"../input/train.json\")\n",
    "    df_test = pd.read_json(\"../input/test.json\")\n",
    "    # df_sample_input = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "    if env == \"prod\":\n",
    "        # set that to some default value\n",
    "        df_test['cuisine'] = \"todo\"\n",
    "        if sample_size is not None and sample_size > 0:\n",
    "            df_data = df_data.sample(sample_size)\n",
    "        else:\n",
    "            df_data = df_data\n",
    "    else:\n",
    "        if sample_size is not None and sample_size > 0:\n",
    "            df_data = df_data.sample(sample_size)\n",
    "        df_test = df_data.sample(test_size)\n",
    "\n",
    "        # Removing all df_test from df_data to ensure not train with test data\n",
    "        df_common = df_data.merge(df_test,on=['id'])\n",
    "        df_data = df_data[(~df_data.id.isin(df_common.id))]\n",
    "    pt(\"end   - read_data\")\n",
    "    return df_data, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_value(myValue, regex):\n",
    "    success = True\n",
    "    result = myValue.lower()\n",
    "    \n",
    "    result = regex.sub('', result)\n",
    "    # Remove trimming s\n",
    "    if result.endswith('s'):\n",
    "        result = result[:-1]\n",
    "    \n",
    "    success = len(result) > 2\n",
    "    \n",
    "    result = \"res_\" + result\n",
    "    \n",
    "    return result, success\n",
    "\n",
    "def preprocess_dataframe(df1, df2, \n",
    "                         column_name = 'ingredients',\n",
    "                         split_by = '\\s+'):\n",
    "    \"\"\"\n",
    "    Given 2 dataframe, extract the column \"\"\n",
    "    \"\"\"\n",
    "    pt(\"start - preprocess_dataframe\")\n",
    "    count1 = len(df1)\n",
    "    count2 = len(df2)\n",
    "    \n",
    "    total_df = df1.append(df2, ignore_index=True)\n",
    "    \n",
    "    d_list = []\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    # REGULAR EXPRESSION FOR CHARACGTER TO REMOVE IN PROCESS VALUE\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    \n",
    "    for index, row in total_df.iterrows():\n",
    "        for value_field in row[column_name]:\n",
    "            # split by space\n",
    "            # values = re.split(split_by, value_field)\n",
    "            # Reverse split by non alphabetical character\n",
    "            values = re.split(r\"[^a-zA-Z]\", value_field)\n",
    "            \n",
    "            current_values = []\n",
    "            for value in values:\n",
    "                \n",
    "                # Remove all weird characters.\n",
    "                processed_value, processed_success = process_value(value, regex)\n",
    "                \n",
    "                # Only add it if it was not already done\n",
    "                if processed_value not in current_values and processed_success:\n",
    "                    # build a dictionnary of all values\n",
    "                    if processed_value not in output:\n",
    "                        output.append(processed_value)\n",
    "\n",
    "                    current_values.append(processed_value)\n",
    "                    d_list.append({'id':row['id'], \n",
    "                                   'value':processed_value})\n",
    "\n",
    "    total_df = total_df.append(d_list, ignore_index=True)\n",
    "    total_df = total_df.groupby('id')['value'].value_counts()\n",
    "    total_df = total_df.unstack(level=-1).fillna(0)\n",
    "    \n",
    "    # Then, we need to merge df_1 and df_2 with their id\n",
    "    df1 = df1.merge(total_df, left_on='id', right_on='id', how='inner')\n",
    "    df2 = df2.merge(total_df, left_on='id', right_on='id', how='inner')\n",
    "    \n",
    "    del(total_df)\n",
    "\n",
    "    # We do not need the ingredients column now, so, we can remove it\n",
    "    df1 = df1.drop(columns=column_name)\n",
    "    df2 = df2.drop(columns=column_name)\n",
    "    pt(\"end   - preprocess_dataframe\")\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_occurence(df1, df2, min_value=0, exclude_columns=['cuisine', 'id']):\n",
    "    \"\"\"Given 2 dataframe, remove all occurence that happen less than x times\"\"\"\n",
    "    pt(\"start - remove_occurence\")\n",
    "    total_df = df1.append(df2, ignore_index=True)\n",
    "    # Removing the ignored columns\n",
    "    total_df = total_df.drop(columns=exclude_columns)\n",
    "    all_columns = list(total_df.columns.values)\n",
    "    column_to_remove = []\n",
    "    for column in all_columns:\n",
    "        total = total_df[column].sum()\n",
    "        #print(column + \" {0}\".format(total))\n",
    "        if total < min_value:\n",
    "            #print(\"remove: {0} for {1}\".format(total, column))\n",
    "            column_to_remove.append(column)\n",
    "    #df1 = df1.drop(columns=column_to_remove)\n",
    "    #df2 = df2.drop(columns=column_to_remove)\n",
    "    print(\"{0} columns left. Removed {1} out of {2} columns.\".format(len(all_columns)-len(column_to_remove),\n",
    "                                                                    len(column_to_remove), len(all_columns)))\n",
    "    pt(\"end   - remove_occurence\")\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(df_data, max_to_add=1000, column_name='cuisine'):\n",
    "    pt(\"start - normalize_input\")\n",
    "    # Multiply the training data set for food where there is not that much data\n",
    "    df_count = df_data.groupby(column_name)[column_name]\n",
    "    max_recipe_count_per_cuisine = df_count.count().max()\n",
    "    # Loop over all cuisine, if while < minimal, add this dataframe, then add a sample of those, to get exactly the same number\n",
    "    print(\"Max value per cuisine = {0}\".format(max_recipe_count_per_cuisine))\n",
    "\n",
    "    for cuisine in df_data.cuisine.unique():\n",
    "        # nb for cuisine\n",
    "        df_cuisine = df_data.loc[df_data[column_name] == cuisine]\n",
    "        recipe_count = df_cuisine.shape[0]\n",
    "        nb_recipe_to_add = max_recipe_count_per_cuisine - recipe_count\n",
    "        print(\"Got {0} value for {1}, need to add {2}\".format(recipe_count, cuisine, nb_recipe_to_add))\n",
    "        tmp_df = None\n",
    "        nb_recipe_to_add = min(max_to_add, nb_recipe_to_add)\n",
    "        if nb_recipe_to_add != 0:\n",
    "            while nb_recipe_to_add != 0:\n",
    "                if nb_recipe_to_add >= recipe_count:\n",
    "                    # Add the full dataframe\n",
    "                    if tmp_df is None:\n",
    "                        tmp_df = df_cuisine\n",
    "                    else:\n",
    "                        tmp_df = tmp_df.append(df_cuisine, ignore_index=True)\n",
    "                    nb_recipe_to_add -= recipe_count\n",
    "                else:\n",
    "                    # Only add a sample of it\n",
    "                    if tmp_df is None:\n",
    "                        tmp_df = df_cuisine\n",
    "                    else:\n",
    "                        tmp_df = tmp_df.append(df_cuisine.sample(nb_recipe_to_add), ignore_index=True)\n",
    "                    nb_recipe_to_add = 0\n",
    "                # Add tmp df to df_data\n",
    "            df_data = df_data.append(tmp_df, ignore_index=True)\n",
    "            print(\"Append a dataframe of {0} values\".format(tmp_df.shape[0]))\n",
    "            del(tmp_df)\n",
    "    pt(\"end   - normalize_input\")\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-15 22:21:21.677008 - start - read_data\n",
      "2018-09-15 22:21:23.929559 - end   - read_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10675</th>\n",
       "      <td>italian</td>\n",
       "      <td>20755</td>\n",
       "      <td>[chili flakes, salt, vegetables, shredded mozz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13157</th>\n",
       "      <td>french</td>\n",
       "      <td>19273</td>\n",
       "      <td>[dried thyme, green onions, ground turkey, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20060</th>\n",
       "      <td>italian</td>\n",
       "      <td>849</td>\n",
       "      <td>[boneless skinless chicken breasts, cayenne pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13403</th>\n",
       "      <td>brazilian</td>\n",
       "      <td>11785</td>\n",
       "      <td>[chicken wings, flour, salt, lime, vegetable o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28769</th>\n",
       "      <td>thai</td>\n",
       "      <td>5133</td>\n",
       "      <td>[whitefish, gluten, extra virgin coconut oil, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cuisine     id                                        ingredients\n",
       "10675    italian  20755  [chili flakes, salt, vegetables, shredded mozz...\n",
       "13157     french  19273  [dried thyme, green onions, ground turkey, chi...\n",
       "20060    italian    849  [boneless skinless chicken breasts, cayenne pe...\n",
       "13403  brazilian  11785  [chicken wings, flour, salt, lime, vegetable o...\n",
       "28769       thai   5133  [whitefish, gluten, extra virgin coconut oil, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-15 22:21:23.947977 - start - preprocess_dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-15 22:21:48.585818 - end   - preprocess_dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>res_abalone</th>\n",
       "      <th>res_abbamele</th>\n",
       "      <th>res_absinthe</th>\n",
       "      <th>res_abura</th>\n",
       "      <th>res_acai</th>\n",
       "      <th>res_accent</th>\n",
       "      <th>res_accompaniment</th>\n",
       "      <th>res_achiote</th>\n",
       "      <th>...</th>\n",
       "      <th>res_yum</th>\n",
       "      <th>res_yuzu</th>\n",
       "      <th>res_yuzukosho</th>\n",
       "      <th>res_zatarain</th>\n",
       "      <th>res_zero</th>\n",
       "      <th>res_zest</th>\n",
       "      <th>res_zesty</th>\n",
       "      <th>res_zinfandel</th>\n",
       "      <th>res_ziti</th>\n",
       "      <th>res_zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31940</th>\n",
       "      <td>indian</td>\n",
       "      <td>46912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38427</th>\n",
       "      <td>italian</td>\n",
       "      <td>7185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>greek</td>\n",
       "      <td>18808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33753</th>\n",
       "      <td>italian</td>\n",
       "      <td>40089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12499</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>24145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2742 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cuisine     id  res_abalone  res_abbamele  res_absinthe  res_abura  \\\n",
       "31940       indian  46912          0.0           0.0           0.0        0.0   \n",
       "38427      italian   7185          0.0           0.0           0.0        0.0   \n",
       "7919         greek  18808          0.0           0.0           0.0        0.0   \n",
       "33753      italian  40089          0.0           0.0           0.0        0.0   \n",
       "12499  southern_us  24145          0.0           0.0           0.0        0.0   \n",
       "\n",
       "       res_acai  res_accent  res_accompaniment  res_achiote      ...       \\\n",
       "31940       0.0         0.0                0.0          0.0      ...        \n",
       "38427       0.0         0.0                0.0          0.0      ...        \n",
       "7919        0.0         0.0                0.0          0.0      ...        \n",
       "33753       0.0         0.0                0.0          0.0      ...        \n",
       "12499       0.0         0.0                0.0          0.0      ...        \n",
       "\n",
       "       res_yum  res_yuzu  res_yuzukosho  res_zatarain  res_zero  res_zest  \\\n",
       "31940      0.0       0.0            0.0           0.0       0.0       0.0   \n",
       "38427      0.0       0.0            0.0           0.0       0.0       0.0   \n",
       "7919       0.0       0.0            0.0           0.0       0.0       0.0   \n",
       "33753      0.0       0.0            0.0           0.0       0.0       0.0   \n",
       "12499      0.0       0.0            0.0           0.0       0.0       0.0   \n",
       "\n",
       "       res_zesty  res_zinfandel  res_ziti  res_zucchini  \n",
       "31940        0.0            0.0       0.0           0.0  \n",
       "38427        0.0            0.0       0.0           0.0  \n",
       "7919         0.0            0.0       0.0           0.0  \n",
       "33753        0.0            0.0       0.0           0.0  \n",
       "12499        0.0            0.0       0.0           0.0  \n",
       "\n",
       "[5 rows x 2742 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data, df_test = read_data(env, sample_size)\n",
    "display(df_data.sample(5))\n",
    "df_data, df_test = preprocess_dataframe(df_data, df_test)\n",
    "display(df_data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-15 22:21:48.741062 - start - remove_occurence\n",
      "785 columns left. Removed 1955 out of 2740 columns.\n",
      "2018-09-15 22:21:50.316914 - end   - remove_occurence\n"
     ]
    }
   ],
   "source": [
    "df_data, df_test = remove_occurence(df_data, df_test, nb_min_value_before)\n",
    "if normalize_training:\n",
    "    df_data = normalize_input(df_data, max_to_add)\n",
    "    df_data, df_test = remove_occurence(df_data, df_test, nb_min_value_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping columns\n",
      "end dropping columns\n",
      "dropping columns\n",
      "end dropping columns\n",
      "dropping columns\n",
      "end dropping columns\n",
      "dropping columns\n",
      "end dropping columns\n"
     ]
    }
   ],
   "source": [
    "# Generate our training/validation datasets\n",
    "from sklearn import model_selection\n",
    "import sklearn\n",
    "\n",
    "# Name of the result column\n",
    "result_cols = ['cuisine']\n",
    "result_excl_cols = ['cuisine_']\n",
    "\n",
    "input_cols = [\n",
    "    'res_'\n",
    "]\n",
    "input_excl_cols = []\n",
    "# Removing input_cols = ['store', 'item',\n",
    "# dom, cw, \n",
    "\n",
    "# Train on everything\n",
    "\n",
    "# Get the final values\n",
    "def get_values(df, cols=[], excl_cols = []):\n",
    "    columns = df.columns.values\n",
    "    # Remove all columns that are not inside the list\n",
    "    cols_to_drop = []\n",
    "    for column in columns:\n",
    "        find = False\n",
    "        ignore = False\n",
    "        for excl_col in excl_cols:\n",
    "            if column.startswith(excl_col):\n",
    "                ignore = True\n",
    "        if ignore is False:\n",
    "            for col in cols:\n",
    "                if column.startswith(col):\n",
    "                    find = True\n",
    "        if not find:\n",
    "            cols_to_drop.append(column)\n",
    "    print(\"dropping columns\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(\"end dropping columns\")\n",
    "    new_order = sorted(df.columns.values)\n",
    "    # Same order for both training and testing set\n",
    "    df = df[new_order]\n",
    "    return df.values\n",
    "\n",
    "df_data_shuffle = sklearn.utils.shuffle(df_data)\n",
    "\n",
    "X_train = get_values(df_data, input_cols, input_excl_cols)\n",
    "X_test = get_values(df_test, input_cols, input_excl_cols)\n",
    "\n",
    "Y_train = get_values(df_data, result_cols, result_excl_cols).ravel()\n",
    "\n",
    "del_object(df_data)\n",
    "# In test env, we calculate it for the test only\n",
    "if env == \"test\":\n",
    "    Y_test = get_values(df_test, result_cols, result_excl_cols).ravel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "\n",
    "X_all = [x + y for x, y in zip(X_train, X_test)]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "# Def adding x_train + X_test + X_validation to fit all of them\n",
    "scaler.fit(X_all)  \n",
    "\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to calculate the SMAPE\n",
    "def get_score(Y_validation, Y_validation_predict):\n",
    "    nb_success = 0\n",
    "    for i in range(0, len(Y_validation)):\n",
    "        if Y_validation[i] == Y_validation_predict[i]:\n",
    "            nb_success += 1\n",
    "    return nb_success / len(Y_validation) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing for model MLPClassifier_adamrelu_earlystopping_5\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_5\n",
      "Model MLPClassifier_adamrelu_earlystopping_5 got score of 76.3, time: 0:00:15.962253\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_6\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_6\n",
      "Model MLPClassifier_adamrelu_earlystopping_6 got score of 76.2, time: 0:00:15.036856\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_7\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_7\n",
      "Model MLPClassifier_adamrelu_earlystopping_7 got score of 75.8, time: 0:00:19.375624\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish fit for MLPClassifier_adamrelu_earlystopping_8\n",
      "Model MLPClassifier_adamrelu_earlystopping_8 got score of 77.60000000000001, time: 0:00:14.427346\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_9\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_9\n",
      "Model MLPClassifier_adamrelu_earlystopping_9 got score of 77.4, time: 0:00:16.440599\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_10\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_10\n",
      "Model MLPClassifier_adamrelu_earlystopping_10 got score of 76.4, time: 0:00:12.214482\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_11\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_11\n",
      "Model MLPClassifier_adamrelu_earlystopping_11 got score of 75.0, time: 0:00:14.852795\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_12\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_12\n",
      "Model MLPClassifier_adamrelu_earlystopping_12 got score of 75.1, time: 0:00:13.991575\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_13\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_13\n",
      "Model MLPClassifier_adamrelu_earlystopping_13 got score of 76.4, time: 0:00:16.381679\n",
      "Executing for model MLPClassifier_adamrelu_earlystopping_14\n",
      "Finish fit for MLPClassifier_adamrelu_earlystopping_14\n",
      "Model MLPClassifier_adamrelu_earlystopping_14 got score of 77.5, time: 0:00:15.207108\n"
     ]
    }
   ],
   "source": [
    "# Import algorithm\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "models = []\n",
    "\n",
    "#models.append(('LogisticRegression', LogisticRegression()))\n",
    "#models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "#models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis()))\n",
    "#models.append(('GaussianNB', GaussianNB()))\n",
    "#models.append(('SVC', SVC()))\n",
    "\n",
    "for i in range(5, 5 + nb_models):\n",
    "    #models.append(('MLPClassifier_adamrelu_{0}'.format(i), MLPClassifier(hidden_layer_sizes=(i,), \n",
    "    #                                                            activation='relu', \n",
    "    #                                                            solver='adam',\n",
    "    #                                                            alpha=0.001, \n",
    "    #                                                            batch_size='auto',\n",
    "    #learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    #random_state=i, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    #early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)))\n",
    "    \n",
    "    # Try multiple solutions\n",
    "    hidden_layers = (50, )\n",
    "    \n",
    "    models.append(('MLPClassifier_adamrelu_earlystopping_{0}'.format(i), MLPClassifier(hidden_layer_sizes=hidden_layers, \n",
    "                                                                activation='logistic', \n",
    "                                                                solver='adam',\n",
    "                                                                alpha=0.001, \n",
    "                                                                batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=i, tol=0.00001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)))\n",
    "#models = []\n",
    "#models.append(('lgbm', lgbm.sklearn.LGBMRegressor()))\n",
    "# High value until first model get solved\n",
    "best_model = \"UNKNOWN\"\n",
    "\n",
    "res = []\n",
    "# Testing all models, one by one\n",
    "for name, model in models:\n",
    "    print(\"Executing for model {0}\".format(name))\n",
    "    time_start = datetime.now()\n",
    "\n",
    "    # Training the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    print(\"Finish fit for {0}\".format(name))\n",
    "\n",
    "    Y_test_result = model.predict(X_test)\n",
    "    res.append(Y_test_result)\n",
    "    if env == \"test\":\n",
    "        # We can calculate the avg error\n",
    "        score = get_score(Y_test, Y_test_result)\n",
    "        print(\"Model {0} got score of {1}, time: {2}\".format(name, score, datetime.now() - time_start))\n",
    "    else:\n",
    "        # Let's write an output file, with the name of the model\n",
    "        print(\"Writing output file {0}.csv for model {0}\".format(name))\n",
    "        \n",
    "        df_test['cuisine'] = Y_test_result\n",
    "        result_df = df_test[['id', 'cuisine']]\n",
    "        result_df['cuisine'] = Y_test_result\n",
    "        \n",
    "        result_df.to_csv(\"{0}.csv\".format(name), index=False)\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg model got score of 78.7\n"
     ]
    }
   ],
   "source": [
    "# For all result in res, if test, display the result, if not, write it to a file\n",
    "final_res = []\n",
    "nb_variable = len(res[0])\n",
    "for variable in range(0, nb_variable):\n",
    "    final_res.append(0.0)\n",
    "    dict_cuisine = {}\n",
    "    for i in range(0, len(res)):\n",
    "        cuisine_found = res[i][variable]\n",
    "        if cuisine_found in dict_cuisine:\n",
    "            dict_cuisine[cuisine_found] += 1\n",
    "        else:\n",
    "            dict_cuisine[cuisine_found] = 1\n",
    "    # Now, we need to find the most common one for all the values inside dict_cuisine\n",
    "    current_value = 0\n",
    "    current_cuisine = \"\"\n",
    "    for cuisine in dict_cuisine:\n",
    "        if dict_cuisine[cuisine] > current_value:\n",
    "            current_cuisine = cuisine\n",
    "            current_value = dict_cuisine[cuisine]\n",
    "    \n",
    "    final_res[variable] = current_cuisine\n",
    "\n",
    "if env == \"test\":\n",
    "    # We can calculate the avg error\n",
    "    score = get_score(Y_test, final_res)\n",
    "    print(\"avg model got score of {0}\".format(score))\n",
    "else:\n",
    "    print(\"Writing output file merged.csv\".format(name))\n",
    "\n",
    "    df_test['cuisine'] = final_res\n",
    "    result_df = df_test[['id', 'cuisine']]\n",
    "    result_df['cuisine'] = final_res\n",
    "\n",
    "    result_df.to_csv(\"merged.csv\".format(name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
